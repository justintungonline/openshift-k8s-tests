# Openshift Kubernetes Tests

Following an Openshift Kubernetes workshop for developers which can be run in [Red Hat Openshift playground](https://www.redhat.com/en/interactive-labs/openshift)

The workshop is on a ParksMap application which is a polyglot spatial data visualization app using a microservices architecture and composed of a set of services using different programming languages and frameworks.

## Learning Outcomes of the Workshop

- Using the OpenShift command-line client and web console.
- Deploying an application using a pre-existing container image.
- Working with application labels to identify component parts.
- Scaling up your application in order to handle web traffic.
- Exposing your application to users outside of the cluster.
- Viewing and working with logs generated by your application.
- Accessing your application container and interacting with it.
- Giving access to other users to collaborate on your application.
- Deploying an application from source code in a Git repository.
- Deploying a database from the OpenShift service catalog.
- Configuring an application so it can access a database.
- Setting up webhooks to enable automated application builds.

Go through building and deploying a microservice architecture. The workshop shows you how OpenShift can be used for deploying stateless web applications and applications which require persistent file system storage. The flexibility makes OpenShift a platform for deploying both web applications and databases.

## Set Up

- Available in Python, Java, and NodeJS. This repository follows the Java and Python options for the application.
- Uses the Openshift web console with administrative access and `oc` terminal commands.
- The workshop is hosted in an OpenShift environment with:
  - Master node(s)
  - Infrastructure node(s)
  - Worker or "application" nodes
  - Dynamic Provisioned Storage
  - The infrastructure node is providing several services of:
    - Gogs git server
    - OpenShift container registry
    - OpenShift router

## Architecture

The workshop is on a ParksMap application which is a polyglot spatial data visualization app using a microservices architecture and composed of a set of services using different programming languages and frameworks.

Components are:

- Parksmap Web, API Gateway, JavaScript
- National Parks Backend REST API, MongoDB
- MLB Parks Backend REST API, MongoDB
- REST API

- Uses code from the [Redhat Roadshow parks repositories](https://github.com/openshift-roadshow/) on GitHub:
  - Front end service users a Docker image of the [parksmap web application](https://github.com/openshift-roadshow/parksmap-web/).
  - The backend services ares called [nationalparks](https://github.com/openshift-roadshow/nationalparks) and [mlbparks](https://github.com/openshift-roadshow/mlbparks).
  - It is a Java Spring Boot application that performs 2D geo-spatial queries against a MongoDB database to locate and return map coordinates of all National Parks and Major League Baseball parks in the world. The backends are webservices that return a JSON list of places.

## Workshop Walkthrough

This README follows the workshop using `oc` commands and the Developer perspective in the Openshift web console unless it is said otherwise and a GitHub repository for code. Instead of GitHub, a self hosted git [Gogs](https://gogs.io/) server can be used.

### Deploy Container

- Log into the web console for graphical management
  - Like http://console-openshift-console.apps.cluster-..../k8s/cluster/projects
  - Switch to developer view and select topology
    - In top right, switch between graph and list views
  - Add a plain docker image: +Add --> Select project -->  Container images
  - In image name, use: quay.io/openshiftroadshow/parksmap:latest
  - Make sure to have the correct values in:
    - Application Name : `workshop`
    - Name : `parksmap`
    - Ensure Deployment is selected from Resource section.
    - Un-check the checkbox next to Create a route to the application. For learning purposes, we will create a Route for the application later in the workshop.
    - At the bottom of the page, click Labels in the Advanced Options section and add some labels to better identify this deployment later. Labels will help us identify and filter components in the web console and in the command line.
    - Add 3 labels. After you enter the name=value pair for each label, press tab or de-focus with mouse before typing the next. First the name to be given to the application.
      - `app=workshop`
      - Next the name of this deployment: `component=parksmap`
      - Role of component in the app: `role=frontend`
    - After deployment, view details of the pod in Administrator view --> Workloads --> Pods

Using oc command in terminal, check application is running and its configuration.

- The oc command can be downloaded from the web console under help (question mark icon) and then choose command line tools. The `oc` command uses `curl` to use the Kubernetes API behind the scenes.
- Keep the web console open on the Topology window of the application to see changes shown as changes are made


```shell

# Openshift CLI oc command tool, display help
oc help

# Switch to project used by this workshop
# will use user50 for sake of this demonstration
oc project user50

# See pods running
oc get pods

# format output in yaml
oc get pods -o yaml
oc get pod <name of pod> -o yaml

# View services - a group of similar pods
# Should map to the parksmap app deployed earlier
oc get services

# See details of service in yaml
oc get service parksmap -o yaml

# Describe the service
oc describe service parksmap

```

#### Example pod, service yaml output, and service description

Note the selector information and labels inputted from the deployment step before:

pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.v1.cni.cncf.io/network-status: |-
      [{
          "name": "openshift-sdn",
          "interface": "eth0",
          "ips": [
              "10.130.16.12"
          ],
          "default": true,
          "dns": {}
      }]
    k8s.v1.cni.cncf.io/networks-status: |-
      [{
          "name": "openshift-sdn",
          "interface": "eth0",
          "ips": [
              "10.130.16.12"
          ],
          "default": true,
          "dns": {}
      }]
    openshift.io/generated-by: OpenShiftWebConsole
    openshift.io/scc: restricted
  creationTimestamp: "2023-11-30T15:55:02Z"
  generateName: parksmap-5ddbdb95cf-
  labels:
    app: parksmap
    component: parksmap
    deploymentconfig: parksmap
    pod-template-hash: 5ddbdb95cf
    role: frontend
  name: parksmap-5ddbdb95cf-tpzd4
  namespace: user50
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: parksmap-5ddbdb95cf
    uid: 9ecf49ed-7a80-4e8a-9811-6a67103cd8fd
  resourceVersion: "1018112"
  uid: fb94bc45-b848-4123-976a-11dff58d041b
spec:
  containers:
  - image: image-registry.openshift-image-registry.svc:5000/user50/parksmap@sha256:89d1e324846cb431df9039e1a7fd0ed2ba0c51aafbae73f2abd70a83d5fa173b
    imagePullPolicy: Always
    name: parksmap
    ports:
    - containerPort: 8080
      protocol: TCP
    resources: {}
    securityContext:
      capabilities:
        drop:
        - KILL
        - MKNOD
        - SETGID
        - SETUID
      runAsUser: 1001330000
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-tbgkm
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  imagePullSecrets:
  - name: default-dockercfg-dccdx
  nodeName: ip-10-0-164-186.us-east-2.compute.internal
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    fsGroup: 1001330000
    seLinuxOptions:
      level: s0:c36,c35
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-tbgkm
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
      - configMap:
          items:
          - key: service-ca.crt
            path: service-ca.crt
          name: openshift-service-ca.crt
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2023-11-30T15:55:02Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2023-11-30T15:55:10Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2023-11-30T15:55:10Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2023-11-30T15:55:02Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: cri-o://c0c228a0312df64c1f7bee77b29b4f656fc389576dcd81160a49230325ce8bba
    image: image-registry.openshift-image-registry.svc:5000/user50/parksmap@sha256:89d1e324846cb431df9039e1a7fd0ed2ba0c51aafbae73f2abd70a83d5fa173b
    imageID: image-registry.openshift-image-registry.svc:5000/user50/parksmap@sha256:89d1e324846cb431df9039e1a7fd0ed2ba0c51aafbae73f2abd70a83d5fa173b
    lastState: {}
    name: parksmap
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2023-11-30T15:55:10Z"
  hostIP: 10.0.164.186
  phase: Running
  podIP: 10.130.16.12
  podIPs:
  - ip: 10.130.16.12
  qosClass: BestEffort
  startTime: "2023-11-30T15:55:02Z"
```

service:

```yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftWebConsole
  creationTimestamp: "2023-11-30T15:55:02Z"
  labels:
    app: workshop
    app.kubernetes.io/component: parksmap
    app.kubernetes.io/instance: parksmap
    app.kubernetes.io/name: parksmap
    app.kubernetes.io/part-of: workshop
    app.openshift.io/runtime-version: latest
    component: parksmap
    role: frontend
  name: parksmap
  namespace: user50
  resourceVersion: "1017894"
  uid: f3e7bfa3-a9de-48f3-b38c-c8db7db5d978
spec:
  clusterIP: 172.30.240.29
  clusterIPs:
  - 172.30.240.29
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: 8080-tcp
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: parksmap
    deploymentconfig: parksmap
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}

```

Service Description

```sh
Name:              parksmap
Namespace:         user50
Labels:            app=workshop
                   app.kubernetes.io/component=parksmap
                   app.kubernetes.io/instance=parksmap
                   app.kubernetes.io/name=parksmap
                   app.kubernetes.io/part-of=workshop
                   app.openshift.io/runtime-version=latest
                   component=parksmap
                   role=frontend
Annotations:       openshift.io/generated-by: OpenShiftWebConsole
Selector:          app=parksmap,deploymentconfig=parksmap
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                172.30.240.29
IPs:               172.30.240.29
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.130.16.12:8080
Session Affinity:  None
Events:            <none>
```


### Scaling and Self Healing, ReplicaSet Management


- Services provide routing and load balancing for pods
- ReplicaSet (RS) and ReplicationController (RC) configure the desired number of Pods (replicas) in existence
- ReplicaSets and ReplicationController are how OpenShift "self heals". Deployments control ReplicaSets, ReplicationController here are controlled by DeploymentConfigs.

Scale parksmap "application" up to 2 instances. We can do this with the `scale` command. You could also do this by incrementing the Desired Count in the OpenShift web console.

```shell

# View the Deployment (D) created by OpenShift with the parksmap image
oc get deployment

# View the ReplicaSet (RS)
oc get rs

# The output will show 1 desired to be deployed and 1 current are deployed
# Scale the app to 2 instances or do it in the topology details in the web console
oc scale --replicas=2 deployment/parksmap

# Verify the scaling of 2 replicas and 2 number of pods
oc get rs
oc get pods

# Verify the service and see the service's endpoints
oc describe svc parksmap
oc get endpoints parksmap

# Delete a pod to see OpenShift immediately create a new one
oc delete pod <pod-name> && oc get pods

# Sca;e back down to 1 replica and verify
oc scale --replicas=1 deployment/parksmap
oc get rs

```

### Exposing Application to the Internet

- Services represent the solution. Users outside of OpenShift can access it using a Route.
- The default OpenShift router (HAProxy) uses the HTTP header of the incoming request to determine where to proxy the connection. You can optionally define security, such as TLS, for the Route. If you want your Services, and, by extension, your Pods, to be accessible from the outside world, you need to create a Route.

Expose a route to the application so users can access it. These steps can also be done in the Administrator and Developer perspectives in the web console under Networking (admin) and Resources (dev)

```shell

# Check no routes exist first
oc get routes
# Get the Service name to expose
oc get services
# Create the route
oc create route edge parksmap --service=parksmap
# Verify route was created, see 8080-tcp was exposed which is from the image Dockerfile
# https://github.com/openshift-roadshow/parksmap-web/blob/master/Dockerfile
oc get route
# In a browser, visit the URL in the HOST/PORT information of the route

```

This application is now available at the URL shown in the Developer Perspective > Topology. Click the link and you will see it.

### Exploring Logs

- Check logs for a pod in the web console with Topology view > pod > Resources tab > View logs. Alernatively, using oc commands
- OpenShift is constructed in such a way that it expects containers to log all information to STDOUT. Informational and error logs is captured via standardized Docker mechanisms. Exploring the logs is going through the Docker daemon to access the container’s logs, through OpenShift’s API.

```shell
oc get pods
oc logs <insert pod name>
```

Notice there is an error in the logs which will be addressed later.

### Role-Based Access Control

- OpenShift’s control plane API are authenticated (AuthN - who are you?) and authorized (AuthZ - are you allowed to do what you are asking?). OpenShift is a declarative platform and some actions are performed by a service account.
- The Openshift default service account is the one taking the responsibility of running the pods, and OpenShift uses and injects this service account into every pod that is launched. The permissions for that service account can be changed.

View current permissions in the web console, go to the Topology view in the Developer Perspective > pod detail > Details tab > Namespace > Role Bindings. The Create binding can be use to modify roles there.

```shell
# Switch to the active project
oc projects
oc project <insert project name>
# Give the default service account view access
# -z, --serviceaccount=[]: service account in the current namespace to use as a user
# use -n <project> for a different project
oc policy add-role-to-user view -z default

# The Service Account will not have view access so it can query the API to see
# project resources and remove the previous error

# Redeploy the application as it has given up trying to query the API
oc rollout restart deployment/parksmap

# Optionally, grant access to your user / another user to view your project
oc policy add-role-to-user view <insert other username>
```

The `oc policy` command above is giving a defined *role* (`view`) to a user. Using a special flag, `-z`, the `-z` syntax saves from having to type out the entire string, which, in this case, is `system:serviceaccount:<project name>:default`. The `-z` flag will only work for service accounts in the current project. If you're referring to a service account in a different project, use the `-n <project>`switch.

After restart, logs will no longer show errors related to the service account.

### Remote Access to Application, Remote Shell Session to a Container Using the CLI

- Containers are treated as immutable infrastructure and it is not recommended to modify the content of a container through SSH or running custom commands inside the container
- Accessing the shell of the running container, for example to troubleshoot issues, debugging, or checking the container is a valid use to access the container
- The web console can access a terminal session in the pod details

```sh
oc get pods
oc rsh <insert pod name>
# Alternate to above using bash shell
oc rsh <insert pod name> /bin/bash
# Check root files
sh-4.2$ ls /
# Exit container shell
sh-4.2$ exit

# To execute commands, you can log into the shell as above or
# use the `oc exec` command. This example shows a JAR file:

oc exec <insert pod name> -- ls -l /parksmap.jar

# Alternatively, oc rsh can also be used in a similar way like
oc rsh <insert pod name> whoami


```

- After `oc rsh` command, you should see a shell prompt `sh-4.2$`. NOTE: The default shell used by `oc rsh` is `/bin/sh`. If the deployed container does not have **sh** installed and uses another shell, (e.g. **A Shell**) the shell command can be specified after the pod name in the issued command.
- The `--` syntax in the `oc exec` command delineates where exec's options end and where the actual command to execute begins. Take a look at `oc exec --help` for more details.
- Note the user is not the user specific in the `Dockerfile`. For security reasons, OpenShift does not run containers as the user specified in the Dockerfile by default. In fact, when OpenShift launches a container its user is actually randomized.
- If you want or need to allow OpenShift users to deploy container images that do expect to run as root (or any specific user), a small configuration change is needed. You can learn more about the [container image guidelines](https://docs.openshift.com/container-platform/latest/openshift_images/create-images.html#images-create-guide-general_create-images) for OpenShift.

### Deploy the Java/Python Code

- A backend service will be deployed developed in Java/Python that will expose 2 REST endpoints to the visualizer application, the parksmap web app  deployed in the previously.
- The application will query for national parks information including coordinates that is stored in a MongoDB database. This application will also provide an external access point, so that the API provided can be directly used by the end user.

#### Source to Image (S2I) with Remote Repository with Java Code

- OpenShift can build images using source code from an existing repository. This is accomplished using the [Source-to-Image project](https://docs.openshift.com/container-platform/latest/openshift_images/using_images/using-s21-images.html).

In Openshift web console, choose topology / +Add > From Git and enter.

- From Git Repo URL: `https://github.com/justunsix/nationalparks.git`. The workshop uses an embedded git server. This walk through uses a GitHub repository instead
- Git Type: Other
- Choose builder image as Java, version 11
- Application Name : workshop
- Name : nationalparks
- In Resource, select Deployment
- In labels, add 3 of them similar to before:
  - `app=workshop`
  - `component=nationalparks`
  - `role=backend`
- Create the from git image

#### Source to Image (S2I) with Embedded Git Server for Repository with Python Code

In Openshift web console, choose topology / +Add > From Git and enter.

- Use the embedded git repo URL
- Git Type: other
- Select python as Builder Image
- Follow rest of steps like the Java instructions above starting with the application name

#### Java/Python Application Verification

- The Java-based application uses Maven as the build and dependency system.
- The initial build will take a few minutes as all of the dependencies needed for the application.

```sh
# Verify S2I is deployed
oc get pods
oc get builds
# View build logs
oc logs -f builds/nationalparks-1
# Verify the build completed and the pod is in running state
oc get pods
# Notice the build pod is completed while the back end pod is now running

# Openshift automatically created a route for the application
# Check the routes and the URLs for each pod in a browser
oc get routes
```

The back end does not have a web interface; however, it will still respond to a browser. Visit the URL listed and add `/ws/info/` to it. You will see a JSON string like:
`{"id":"nationalparks","displayName":"National Parks","center":{"latitude":"47.039304","longitude":"14.505178"},"zoom":4}`

The JSON confirms the back end is running

### Adding a Database (MongoDB)

- This step will deploy a MongoDB database that will be used to store the data for the nationalparks application.
- It will also connect the nationalparks service with the newly deployed MongoDB database, so that the nationalparks service can load and query the database for the corresponding information.
- The nationalparks application will be marked as a backend for the map visualization tool, so that it can be dynamically discovered by the parksmap component using the OpenShift discovery mechanism and the map will be displayed automatically.
- Most useful applications are "stateful" or "dynamic" and usually achieved with a database or other data storage. - - This step will add a MongoDB to the nationalparks application and configure it to talk to the database using environment variables via a secret.
- It will use the  MongoDB image that is included with OpenShift.
- By default, this will use EmptyDir for data storage, which means if the Pod disappears the data does as well. In a real application you would use OpenShift’s persistent storage mechanism to attach real-world storage (NFS, Ceph, EBS, iSCSI, etc) to the Pods to give them a persistent place to store their data.

#### Templates and Create a Database

- Create a MongoDB template inside the project, so that is only visible to our user and we can access it from Developer Perspective to create a MongoDB instance.
- Use a Template to allow OpenShift to automatically generate things for deployments

```shell

oc create -f https://raw.githubusercontent.com/openshift-labs/starter-guides/ocp-4.8/mongodb-template.yaml -n <project-name>

```

In the Developer Perspective click **+Add** and then **Database**. In the Databases view, you can click **Mongo** to filter for just MongoDB (Make sure to uncheck **Operator Backed** option from **Type** section). Click **Instantiate Template** button
Alternatively, you could type `mongodb` in the search box. Once you have drilled down to see MongoDB, find the **MongoDB (Ephemeral)** template and select it. Normally, in a real application persistent storage is required, but this workshop uses temporary storage.

- You can see that some of the fields say "generated if empty". This is a feature of Templates in OpenShift. For now, be sure to use the following values in their respective fields:

- Leave namespace settings at defaults
- `Database Service Name` : `mongodb-nationalparks`
- `MongoDB Connection Username` : `mongodb`
- `MongoDB Connection Password` : `mongodb`
- `MongoDB Database Name`: `mongodb`
- `MongoDB Admin Password` : `mongodb`

These values are for workshop purposes and should be changed for a real deployment of a database. Click create, then go to secrets.

#### Configure Parameters and Check Replicas

In secrets, find the one with name `mongodb-ephemeral-parameters....`. Click the secret name and it will be use for **Parameters**. The secret can be used in other components, such as the `nationalparks` backend, to authenticate to the database. The connection and authentication information is stored in a secret in our project. Add it to the `nationalparks` backend. Click the **Add Secret to Workload** button. Select the `nationalparks` workload and click **Save**.

In Topology in the web console, move the mongodb into the workshop area in case it is not already there.

``` shell

# Add labels to the `mongodb-nationalparks` deployment and check it
oc label dc/mongodb-nationalparks svc/mongodb-nationalparks app=workshop component=nationalparks role=database --overwrite

# Check the replica set and observe a new version of the nationalparks pod was deployed. The new version is due to the changes in secrets made in the previous steps.
oc get rs
# The old versions are listed as Desired: 0, Current: 0.

```

#### Load data

Check data in the application

```sh
# Get back end URL
oc get routes | grep nationalparks
```

Visit the URL and add `/ws/data/all` to it (e.g. nationalparks-testuser.apps.cluster-sandbox.opentlc.com/ws/data/all). There will be no data, so let's load the data using the same URL except change the context to `ws/data/load` and then this message is displayed `Items inserted in database: 2893`.

This [`MongoDBConnection.java`](http://www.github.com/openshift-roadshow/nationalparks/blob/master/src/main/java/com/openshift/evg/roadshow/parks/db/MongoDBConnection.java#L44-l48) from the back end project shows how the database connection works. It took the environment variable to connect to the database. Environment variables, ConfigMaps, and secrets allow portability without changing application code.

#### Labels and Back end connectivity

- **Labels** help the application understand routes and services.  If any of them have a **Label** that is `type=parksmap-backend`, the application checks endpoints to look for map data.
  - Java example is [`RouteWatcher.java`](https://github.com/openshift-roadshow/parksmap-web/blob/master/src/main/java/com/openshift/evg/roadshow/rest/RouteWatcher.java#L19) in the web front end.
  - Python example is [`wsgi.py`](https://github.com/openshift-roadshow/nationalparks-py/blob/master/wsgi.py#L11-L18)

Check labels on the `nationalparks` route and add a label so it is designated as a back end. That way the front end will know its backend by the label.

```sh
# Check existiong rout
oc describe route nationalparks
# Add label so parksmap know it is the backend
oc label route nationalparks type=parksmap-backend
```

Use `oc get routes` to get the parksmap URL or check it in the web console. Check the front end parksmap URL and you will see points coming up on the map.

The `nationalparks` route will look like:

```shell
Name:                   nationalparks
Namespace:              user50
Created:                3 hours ago
Labels:                 app=workshop
                        app.kubernetes.io/component=nationalparks
                        app.kubernetes.io/instance=nationalparks
                        app.kubernetes.io/name=nationalparks
                        app.kubernetes.io/part-of=workshop
                        app.openshift.io/runtime=python
                        app.openshift.io/runtime-version=3.8-ubi7
                        component=nationalparks
                        role=backend
                        type=parksmap-backend
Annotations:            openshift.io/host.generated=true
Requested Host:         nationalparks-user50.apps.cluster-zbt47.zbt47.sandbox1620.opentlc.com
                           exposed on router default (host router-default.apps.cluster-zbt47.zbt47.sandbox1620.opentlc.com) 3 hours ago
Path:                   <none>
TLS Termination:        <none>
Insecure Policy:        <none>
Endpoint Port:          8080-tcp

Service:        nationalparks
Weight:         100 (100%)
Endpoints:      10.131.10.38:8080
```

### Application Health: Add Readiness and Liveness Probes

- A liveness probe checks if the container in which it is configured is still running. If the liveness probe fails, the kubelet kills the container, which will be subjected to its restart policy.
- Set a liveness check by configuring the `template.spec.containers.livenessprobe` stanza of a pod configuration. A readiness probe determines if a container is ready to service requests. If the readiness probe fails acontainer, the endpoints controller ensures the container has its IP address removed from the endpoints of all services.
- A readiness probe can be used to signal to the endpoints controller that even though a container is running, it should not receive any traffic from a proxy. Set areadiness check by configuring the `template.spec.containers.readinessprobe` stanza of a pod configuration.

See [Application Health](https://docs.openshift.com/container-platform/latest/applications/application-health.html) documentation for details.

In the web console: Topology > Select nationalparks > Actions: add/edit Health Checks

Add a Readiness Probe using Path: `/ws/healthz/`. Repeat for Liveness Probe with the same path. Leave all defaults as they are such as HTTP GET and port 8080. Check mark both probes and save the health checks page. The save will trigger new deployments of the pods.

### Continuous Integration and Pipelines

A continuous delivery (CD) pipeline is an automated expression of your process for getting software from version control right through to your users and customers. Every change to your software (committed in source control) goes through a complex process on its way to being released. This process involves building the software in a reliable and repeatable manner, as well as progressing the built software (called a "build") through multiple stages of testing and deployment.

OpenShift Pipelines is a cloud-native, continuous integration and delivery (CI/CD) solution for building pipelines using [Tekton](https://tekton.dev/) which automates deployments across platforms.

Custom Tekton resources are:

- Task: a reusable, loosely coupled number of steps that perform a specific task
- Pipeline: a collection of tasks that are run sequentially to define automated steps
- TaskRun: run and result of a task
- PipelineRun: run and result of a pipeline that includes a number of TaskRuns. PipelineRuns are managed by Tekton Controllers

#### Deploy Pipeline

Deploy the pipeline stored in the code repository and verify the tasks and pipelines are present

```sh

oc create -f https://raw.githubusercontent.com/justintungonline/nationalparks/master/pipeline/nationalparks-pipeline-all-new.yaml -n <namespace>
# Verify tasks are available in the cluster
oc get tasks -n <namespace>
oc get pipelines -n <namespace>
```

In the web console, view the pipelines under the Pipelines menu and see 4 Tasks defined for a Java application:

- **git clone**: this is a `ClusterTask` that will clone source repository for nationalparks and store it to a `Workspace` `app-source` which will use the PVC created for it `app-source-workspace`
- **build-and-test**: will build and test our Java application using `maven` `ClusterTask`
- **build-image**: will build an image using a binary file as input in OpenShift. The build will use the .jar file that was created and a custom Task for it `s2i-java11-binary`
- **redeploy**: it will deploy the created image on OpenShift using the Deployment named `nationalparks` we created in the previous lab, using the custom Task `redeploy`

The pipeline will take parameters.

#### Run Pipeline

In the web console, pipeline detail, click Actions > Start:

- When prompted with parameters to add the Pipeline, add in **APP_GIT_URL** the `nationalparks` repository: `https://github.com/justunsix/nationalparks.git`
- In **Workspaces**→ **app-source** select **PVC** from the list, then select **app-source-pvc**. This is the shared volume used by Pipeline Tasks in your Pipeline containing the source code and compiled artifacts.
- Leave Maven settings, it will use an **EmptyDir** volume for the maven cache, this can be extended also with a PVC to make subsequent Maven builds faster

Start the pipeline. In the web console, observe it running. Clicking on a task will show its logs. Verify the pipeline completes.

#### Automation on Code Changes

- Most Git repository servers support web hooks  —  calling to an external source via HTTP(S) when a change in the code repository happens.
- OpenShift has an API endpoint that can receive hooks from remote systems in order to trigger builds. Pointing the code repository’s hook at the OpenShift Pipelines resources allows automated code/build/deploy pipelines.

##### Pipeline Triggers

Tekton triggers enable configuring of pipelines to respond to events like git push events. Adding triggering support requires:

- TriggerTemplate: a trigger template is a template for newly created resources. It supports parameters to create specific `PipelineResources` and `PipelineRuns`.
- TriggerBinding: validates events and extracts payload fields
- EventListener: connects `TriggerBindings` and `TriggerTemplates` into an addressable endpoint (the event sink). It uses the extracted event parameters from each TriggerBinding (and any supplied static parameters) to create the resources specified in the corresponding TriggerTemplate. It also optionally allows an external service to pre-process the event payload via the interceptor field.

Create a new Pod with a Route that can be used to setup a Webhook on a git server to trigger the automatic start of the Pipeline

```sh
oc create -f https://raw.githubusercontent.com/justintungonline/nationalparks/master/pipeline/nationalparks-triggers-all.yaml -n <namespace>

# Verify a new pod el-nationalparks is created as the EventListener
oc get pods
# Get the URL of the Event listener and copy it
oc get routes | grep el-nationalparks
```

Using the URL copied or get the URL from the web console Topology, the URL should something like:
<http://el-nationalparks-user.sandbox.opentlc.com/>

Make sure there is a `/` at the end.

##### Web Hooks

On the GitHub website, go to your `nationalparks` repository and add a new Webhook with Settings > Webhooks > Add.

- Paste the el-nationalparks URL into the webhook URL and change the `Content Type` to `application/json`.
- Leave the secret token field blank
- Choose `Just the push event`
- Add the webhook

New pushes to the `nationalparks` repository will now trigger a new build and deploy in OpenShift.

##### Trigger the Web Hook

In GitHub, go to your `nationalparks` repository and this file `src/main/java/com/openshift/evg/roadshow/parks/rest/BackendController.java`

Change line number 20:

```java
return new Backend("nationalparks","National Parks", new Coordinates("47.039304", "14.505178"), 4);
```

To

```java
return new Backend("nationalparks","Amazing National Parks", new Coordinates("47.039304", "14.505178"), 4);
```

Note the change in name with `Amazing`. Commit the change and in Openshift observe a new PipelineRun should be triggered. In the web console, click Pipeline in the left navigation menu then `nationalparks-pipeline`. You should see a new one running or run the following command to verify:

`oc get PipelineRun`

After the pipeline is completed, verify the change at the nationalparks URL similar to  `nationalparks-user.sandbox.opentlc.com/ws/info/`. Using the `/ws/info` context. The JSON returned will show:

```json
...
displayName "Amazing National Parks"
...
```

To see this `Amazing National Parks` text change in the parksmap’s legend itself, scale down the parksmap pod to 0, then back up to 1 to force the app to refresh its cache:

```sh
oc scale --replicas=0 deployment/parksmap && oc scale --replicas=1 deployment/parksmap
# Wait for parksmap to reload and verify it is running
oc get pods
```

The parksmap may take a few minutes to start up and load the new legend and data points again from the backend, then you can visit the parksmap URL.

#### Application Templates

Next is to deploy a map of Major League Baseball stadiums in the US by using a template. It is pre-configured to build the back end Java application, and deploy the MongoDB database. It also uses a **Hook** to call the `/ws/data/load` endpoint to cause the data to be loaded into the database from a JSON file in the source code repository.

Start using the template:

```sh
# Create the template in the project, a copy is stored in this workshop repository
oc create -f https://raw.githubusercontent.com/openshift-roadshow/mlbparks/master/ose3/application-template-eap.json
# Verify both the mlbparks and MongoDB (imported earlier) templates are available
oc get template
# Instantiate the template
oc new-app mlbparks -p APPLICATION_NAME=mlbparks
# View the mlbparks template
oc get template mlbparks -o yaml
```

The template can also use Maven for the build. This option is available if you provide the **MAVEN_MIRROR_URL** parameter with the location of a nexus repository like: `oc new-app mlbparks --name=mlbparks -p MAVEN_MIRROR_URL=http://nexus.labs.svc.cluster.local:8081/repository/maven-all-public`

This template completed the following in one command:

- Configure and start a build
  - Using the supplied Maven mirror URL (if you have specified the parameter)
  - From the supplied source code repository
- Configure and deploy MongoDB
  - Using auto-generated user, password, and database name
- Configure environment variables for the app to connect to the DB
- Create the correct services
- Label the app service with `type=parksmap-backend`

In the web console Topology view, put `mlbparks` and `mongodb-mlbparks` into the `workshop` application grouping if it isn't already there.

Another way to instantiate the template is using the web console Developer Perspective (**+Add** > **From Catalog** > search for `mlb`. Use result for `MLBparks` > Go through instantiation form. This step is not required as it was done on the command line already.

The template YAML can also be viewed/edited in the web console Developer Perspective: **Advanced → Search** in the left navigation > select **Template** > **mlbparks**.

### Binary Builds for Day to Day Development

Sometimes S2I is too slow for daily iterative development due to the full build and deploy. A faster option is to use `oc` command to use local code for S2I.

This pattern can be used to send up your working directory to the S2I builder to have it do the Maven build on OpenShift. Using the local directory would relieve you from having Maven or any of the Java toolchain on your local machine AND would also not require git commit with a push. Read more in the [official documentation](https://docs.openshift.com/container-platform/latest/dev_guide/dev_tutorials/binary_builds.html).

#### Using Binary Deployment

The next steps will clone the `mlbparks` source to the local environment, build it locally after a change, and use `oc` to use local binaries for the deployment.

```sh
# Clone source in a temporary directory
mkdir temp
cd temp
git clone https://github.com/justunsix/mlbparks.git
cd mlbparks
# Build with Maven, get the output location for the .../mlbparks/target/ROOT.war, it will be needed for the oc command later
# Build application with Maven, it will take a while as Maven donwloads dependencies and then compiles the application
mvn package
```

In GitHub website or your editor of choice, change the source code in the `mlbparks` repository `src/main/java/com/openshift/evg/roadshow/rest/BackendController.java`. This is the REST endpoint that gives basic info on the service.

Please change line 23 to add a *AMAZING* in front of "MLB Parks" look like this:

```java
return new Backend("mlbparks", "AMAZING MLB Parks", new Coordinates("39.82", "-98.57"), 5);
```

Save the file and build the source again or edit it remotely and commit to the repository, then use `git pull`.

```sh
# Re-build the WAR file
mvn package
# Use oc to use the local WAR file
# The --follow is optional if you want to follow the build output in your terminal.
oc start-build bc/mlbparks --from-file=target/ROOT.war --follow
```

Using labels and a recreate deployment strategy, as soon as the new deployment finishes the map name will be updated. Under a recreate deployment strategy we first tear down the pod before doing our new deployment. When the pod is torn down, the parksmap service removes the MLBParks map from the layers. When it comes back up, the layer automatically gets added back with our new title. This would not have happened with a rolling deployment because rolling spins up the new version of the pod before it takes down the old one. Rolling strategy enables a zero-downtime deployment.

Follow the deployment in progress from Topology view, and when it is completed, you will see the new content at the `mlbparks` backend `/ws/info/` URL.

## Configuration Management

### How to pass things into a container?

Options:

- Mount a volume into container
- Set environment variables, using yaml, example is Java processes can use system variables
- ConfigMaps can configure these items
- In web console > pod detail view > Environment tab. Choose environment information and secrets. Database creation steps above are an example of passing variables to the container.

Pod yaml example:

```yaml
...
spec:
    containters:
        - name: parksmap
        ...
        env:
            - name: testvariable
              value: testvalue
...
```

#### [ConfigMaps](https://docs.openshift.com/container-platform/4.6/authentication/configmaps.html)

 It contains key value pairs to store configurations as read only. It is like properties.

Example `yaml`

```yaml
kind: ConfigMap
...
data:
    example.property.1: hello
    settings.json: |-
        {
            # json data with key value pairs
        }
...
```

#### Secrets

Secrets are encoded. Example `yaml`

```yaml
kind: Secret
...
data:
    database-name: aasd2tgs=
    database-password: dXNgfdgepg==
...
```

### How does the front end find the back end?

The front end can query the Kubernetes API to find backends. Finding is done in the application code such as this [`RouteWatcher.java`](https://github.com/openshift-roadshow/parksmap-web/blob/master/src/main/java/com/openshift/evg/roadshow/rest/RouteWatcher.java)

## Further Resources

- **[OpenShift Interactive Learning Portal](https://learn.openshift.com/)** - An online interactive learning environment where you can run through various scenarios related to using OpenShift.

The online interactive learning environment is always available so you can continue to work on those exercises even after the workshop is over.

Below you will find further resources for learning about OpenShift and running OpenShift on your own computer, as well as details about OpenShift Online or other OpenShift related products and services.

- **[OpenShift Container Platform](https://www.openshift.com/)** - The Red Hat supported OpenShift product for installation on premise or in hosted cloud environments.

- **[OpenShift Documentation](https://docs.openshift.com)** - The landing page for OpenShift documentation.
- **[OpenShift Resources on developers.redhat.com](https://developers.redhat.com/openshift/)** - A collection of resources for developers who are building and deploying applications on OpenShift.

- **[OpenShift Local](https://developers.redhat.com/products/openshift-local/overview)** - Install a local OpenShift cluster on your own computer
- **[OpenShift Online](https://manage.openshift.com/)** - A shared public hosting environment for running your applications using OpenShift.
- **[OpenShift Dedicated](https://www.openshift.com/dedicated)** - A dedicated hosting environment for running your applications, managed and supported for you by Red Hat.
- **[OpenShift Commons](https://commons.openshift.org)** - A community for users, partners, customers, and contributors to come together to collaborate and work together on OpenShift.

The following free online eBooks are also available for download related to OpenShift.

- **[OpenShift for Developers](https://www.openshift.com/for-developers/)**
- **[DevOps with OpenShift](https://www.openshift.com/devops-with-openshift/)**
- **[Deploying with OpenShift](https://www.openshift.com/deploying-to-openshift/)**
